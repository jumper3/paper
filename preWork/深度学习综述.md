# 深度学习综述



> LeCun Y, Bengio Y, Hinton G. Deep learning[J]. nature, 2015, 521(7553): 436. 
>
> 翻译来自：https://blog.csdn.net/anysky___/article/details/51164446



###摘要

深度学习是允许由多个处理层和多个抽样层学习数据表示的计算机模型。**这些方法可以显著提高了最先进的语音识别,视觉物体识别、目标检测和许多其他领域如药物发现和基因组学等领域.** **深度学习使用反向传播（BP）算法发现了在大型数据集中的复杂结构来说明如何机器应该如何改变其内部参数用于计算每一层对于上一层的表示。**深度卷积网络在图像处理、视频、语音和音频领域带来了突破，而递归网络对于连续的数据如文本和语音有很好的性能。



机器学习的技术力量涉及现代社会的许多方面：从Web搜索到社交网络的内容过滤再到电子商务网站的建议，它被越来越多呈现在消费者产品中如相机和智能手机。机器学习系统被用于确定图像中的目标，语音到文本转换,匹配新闻元素、根据用户的兴趣提供职位或产品,并选择相关的搜索结果。渐渐地,这些应用程序使用一个叫做深度学习的技术。 　　



传统的机器学习在处理原始数据时的能力是有限的。几十年来，构造一个模式识别或机器学习需要一个精致的工程和相当专业的知识去设计一个特征提取器来完成从原始数据到一个合适的内部表示或特征向量的转换，从这个当中，学习系统，通常是一个分类器，可以完成对输入样本的检测或分类。 　　



表示学习是一组学习方法，它允许向机器输入原始数据，并自动的发现用于检测或分类所需的特征表示。**深度学习方法就是一种拥有多层表示的特征学习方法，在每一层中，它通过简单但是非线性的模块将原始数据转换为更高层次，更抽象的表达。通过足够多的转换，即使非常复杂的而函数也可以被学习。**对于分类任务,更高层次的表示可以放大输入的区别并且抑制无关的变化。例如，对于一幅图像来说，以像素值矩阵的形式输入，在第一层中被学习的特征往往是在图像中某些特定方向或位置边缘存在与否的问题。第二层往往是根据边界的安排检测图案，而忽略边缘位置的细小变化。第三层可能是将图案组合成更大的组合从而与熟悉目标的额某部分相对应。并且随后的层会将这些部分再联合从而构成检测目标。深度学习的关键方面在于，这些层的设计不是由人类工程师完成：它们是通过使用通用的学习算法从原始数据中学习的得到。 　　



深度学习在阻碍人工智能发展的问题解决方面做出了重大进步。它被证明是非常**善于发现高维数据的复杂结构**，因此适用于很多科学领域、商业和政府。它打破了图像识别、语音识别和许多机器学习领域的记录，如预测潜在药物分子的活性，分析颗粒加速数据，重建大脑回路，预测非编码基因序列突变对基因表达和疾病的影响。或许更令人惊讶的是，深度学习在自然语言理解方面的各种课题也产生了很好的效果，特别是主题分类，情感分析，自动问答和语言翻译。 　　



我们认为深度学习将有更大的成功在不远的而将来，因为他需要很少的人工干预，可以很容易的利用计算量和数据的增长。目前正在为深层神经网络而开发的新的学习算法和结构只会加快这一进程。 　　



#### 监督学习　　

最常见的机器学习是，不管其深度与否，有监督的学习。试想我们建立一个系统可以对输入的图片进行分类，这些图片包括人、车子、房子、宠物等等，每个图片对应一个标签，即它的种类。在训练中，机器被给与一个图片，就会产生一个向量形式的输出，它包含了这个图片对应各个类分数（在这里我觉得很像隶属度的概念），我们的目的是对每一个图片来说，想让期望的类对应更高的的分数，如此，我们我们的目标函数，通常是误差函数会有最小的值。机器是通过修改它内部可调节的参数而使得输出值与期望值的差更小的，而这个可调节的参数我们称之为**权重**，是真正人眼可见的“旋钮”，决定了输入与输出之间的关系。在典型的深度学习系统中，有数百万个这样的权重和带有标签的样本用以训练的机器。学习算法计算每个权值的梯度向量，表示了如果权值增加了一个很小的量，那么误差会增加或减少的量。权值向量然后在梯度矢量的相反方向上进行调整。我们的目标函数，对所有样本的平均，可以看作是一种在权值的高维空间上的多变地形，负的梯度向量方向下降最快，也就最容易取得误差的最小值。 　　



在实际中，大部分开发者都会采用**随机梯度下降的方法（SGD）**。它包括提供一小组训练样本的输入向量，计算输出和误差和样本的平均梯度，并且据此调整权重。这个过程被重复通过不断的从训练样本中获取部分训练样本，直到目标函数的平均值不再下降。它被称为随机的，因为小的样本集对于全体样本的平均梯度来说会有噪声估计，相较于一些精致的最优化方法，这个简单的过程通常会很快的而找到一组好的权重。在训练完成后，系统的性能通过测试集来测试，即测试系统的泛化能力——对于新样本的识别能力。 　　



许多当前实际应用中的机器学习使用线性分类器对人手工提取的特证进行分类，一个二级的线性分类器计算特征向量的加权和，若高于阈值，则输入给与一个特定的标签。



自从1960s以来，我们知道线性分类器仅仅通过一个超平面将输入空间分成几个简单的区域。但是图像或语音识别问题等要求输入—输出函数是对输入的不相干变量是不敏感的，例如位置变化，目标的光照或方位，语音中音高或嗓音的变化，而能对一些特定的微小变化敏感。例如我们希望能够区分相同位置相似背景下的白色狗和狼,而线性分类器不能很好的完成。这就是什么浅层分类器要求好的特征提取能力，用于解决选择不变性问题——提取器会挑选出图像中能够区分目标的那些重要因素，但是这些因素对于分辨动物的位置就无能为力了。为了使分类器更强大,一个人可以使用泛化的非线性特征，如核方法,但是泛化特征如通过高斯核产生的，不能使学习者得到对于所有的训练样本都很好的泛化效果。传统的方法是人工设计好的特征提取器,这就需要大量的工程技能和专业知识。**通过使用泛化目标的学习过程，可以自动学习好的特征，从而避免上述问题。这是深度学习的关键优势**。 　　



一个深度学习架构是简单模块多层堆叠,其所有(或大多数)的目标是学习，许多是计算非线性输入输出映射。每个模块的堆叠将转换其输入从而增加选择性和不变性的表示。系统有多个非线性层,通常有5到20层 ,可以实现非常复杂的函数,在对细节敏感的同时，对大型不相关的变量不敏感如背景,姿势,照明和周围的物体。 



#### 训练多层结构的反向传播 　　

在早期的模式识别，研究的目标是期望用训练好的网络代替人工提取特征，尽管它很简单，但是它的解决方法——随机梯度下降法直到1980s中期才被理解。只要每个模块对于它的输入和内部权重是相对平滑的函数，就可以通过反向传播方法进行计算。 　　



用于计算目标函数对于多层模块中的权重反向传播方法仅仅是偏导中链式规则的一个实际应用。关键在于目标对于一个模块输入的导数通过目标对于上一层模块输出的导数得到。（如图1d）从输出层开始，重复反向传播直到回到输入层，一旦得到这些梯度，可以很简单的计算每个模块权重的梯度。 　　



深度学习的许多应用使用前向神经网络结构（图1c），这种结构将固定大小的输入（如一幅图像）映射到固定大小的输出（如对于若干的标签的可能值）。在层间传递中，计算来自于上一层输入的的加权和，并通过非线性函数得到输出。神经网络中的非线性函数包括近几年使用的ReLU函数 ，这个函数可以很快的学习多层网络，也可以训练有监督的深度网络（而不需要无监督的预训练）。 

![img](http://images2015.cnblogs.com/blog/920180/201604/920180-20160415172834973-725843324.png)

传统的sigmoid函数 

![img](http://images2015.cnblogs.com/blog/920180/201604/920180-20160415172955207-1808981957.png)

输入与输出之间的节点通常称为隐藏节点。隐形层可以以一种非线性方式扭曲输入空间。（图1a）图一：多层神经网络和反向传播 ![img](http://images2015.cnblogs.com/blog/920180/201604/920180-20160415194048488-543794640.png)

- a 一个多层神经网络（相连接的节点）通过扭曲输入空间可以使数据集（红线和蓝线代表的样本）更加线性可分。注意输入空间的规则网格（左侧）是如何被隐藏层（中间）转换的。这是一个带有两个输入节点，两个隐藏节点和一个输出节点的示例。但是用于目标识别或自然语言处理的网络通常有成千上百个这样的节点。 
- b x的微小变化量Δx首先会通过乘以∂y/∂x（偏导数）转变成y的变化量Δy。类似的，Δy会给z带来改变Δz。通过链式法则，Δx通过乘以∂y/∂x和∂z/∂x得到Δz。当x，y，z是向量的时候，可以同样处理（使用雅克比矩阵）。 　　
- c 在带有两个隐形层和一个输出层的神经网络的前向通路中使用这些等式，每个包含一个可以反向传播梯度的模块。在每一层中，我们首先计算每一个节点的总输入z，即上一层输出的加权和，将非线性函数作用于z就得到这个节点的输出。 　　
- d 计算反向传播的等式 在每一个隐形层，我们计算误差对于每一个节点输出的偏导，它是误差对上一层输入偏导的加权和。我们通过乘以f（z）的梯度将误差对输出的偏导转换成对输入的导数。在输出层，误差对于每一个节点输出的偏导通过目标函数求导计算。一旦∂E/∂zk 知道，则误差E对权重的导数 



在1990s晚期，神经网络或BP算法被大多数机器学习团体遗弃，也被计算机视觉和和语音识别领域忽视。人们广泛认为学习有用的、多阶段的、不需要先验知识的特征提取器是不可行的。特别的，人们认为简单的梯度下降很可能陷入极小值误区（即局部最小解）。



在实践中，局部最小解在大型网络中很少能成为问题。忽略初始条件，系统总能得到效果差不多的解。最近的理论和经验结果都表明局部最小解不是大问题。正相反，解空间中存在大量的鞍点（即梯度为0的点）。在大量数据中，分析表明鞍点周围只有很少的曲面方向是向下，但是他们中大部分的点的目标函数有相似的值，因此，即使算法陷入这些点也没有大问题。



2006年前后，CIFAR(加拿大高级研究院)聚集了一些研究员，他们使深度前向网络重新获得了生机。研究者及介绍了**无监督学习方法，它可以创建多层特征检测而不需要标签数据**。每一层的特征检测器的目标是能够重建特征检测的活动或者是原始数据。通过**预训练**更加复杂的若干层，网络的权重可以被初始化为合适的数据。输出层被放置在网络的顶部，整个网络可以通过bp算法做出的调整。这个工作在手写体识别和行人检测方面取得了很好的效果，尤其当类别标签数据被限制时。



这种预训练的方法的一个主要应用是语音识别，为程序方便，它是在GPU上做的，整个的训练加速了10到20倍。在2009年这种方法被使用去映射短时间的系数窗口，它提取自一段声波到一系列的语音碎片的的概率值的过程。它打破了在使用较小的词表的标准语音识别基准上的记录，并很快扩展到更大的数据集。到2012年，深度网络的版本已经许多主要的语音团体发展，并应用安卓手机上。对于小数据集来说，无监督预训练可以很好地预防过拟合，取得更好的泛化效果当有标签的数据集很小时。一旦深度学习获得恢复，这种预训练只在数据较少的时候才需要。



然而，有一种特殊的前向网络相对在相邻层实现全连接的网络来说训练更容易，泛化性能更好，它就是卷积神经网络（ConvNet）。在神经网络不受偏爱期间，它获得了许多实践成功，并且已经被计算机视觉团体广泛接受。



#### 卷积神经网络

ConvNets被设计用来处理多重矩阵的数据，例如，一个儿彩色图像由3个2D矩阵组成。许多数据形态是多重矩阵的形式：1D信号和序列包括语音，2D图像或声谱图，3D视频或立体图像。卷积网络利用4个关键点来利用自然信号的属性：局部连接，权值共享，池化和多层结构。如图2一个卷积网络的内部

![img](C:\Users\Administrator\Desktop\cnn.png)

一个应用于Samoyed dog的图像（左边底部，RGB输入，右边底部）典型的卷积网络的每一层（水平的）的输出（不是滤波器）。每一个矩形图像是一个feature map 



典型的卷积网络的结构（图2）由一系列阶段构成。首先的阶段由卷积层和池化层组成，卷积的节点组织在特征映射块（feature maps）中，每个节点与上一层的feature maps中的局部块通过一系列的权重即过滤器连接。加权和的结果被送到非线性函数中如ReLU。一个feature maps中所有的节点分享相同的过滤器，即共享权重。这种结构的原因是双重的，第一,图像中一个值附近的值是高度相关的，第二，不同区域的值是不相干的。换句话说，一个图像某部分出现的特征会在其他部分出现，因此可以实现权值共享并且检测到相同模式在矩阵的不同部分，这种操作在数学上称之为卷积，这也是其名字由来。 　　



**卷积层的功能是检测前一层的特征的局部连接，池化层的作用是是合并特征相似的地方。**这是因为形成一个主题的特征的相对位置会有所不同。一个典型的池化单元计算一个局部块的最大值，邻近的池化单元按照一行或一列或者更多的方式移动的从局部块中获取数据，从而减少了维数，增加了移动或扭曲的不变性。2到3个卷积层，非线性函数和池化层堆叠，再连接上全连接层，通过bp算法训练所有过滤器的权重，就得到了ConvNet。 　　



深度神经网络开发自然信号层级组成的特性，其中，高水平的特征由低水平的组合获得。在图像中，边缘的局部组合形成图案，图案构成部分，部分组成目标。相似的结构存在于语音和文本中，如电话里的声音，音位，音节，文本里的单词和句子。当前一层的元素的位置或外貌变化时，池化能保证表示几乎不变。 　　



卷积网络中卷积层和池化层是由视觉神经科学中的简单细胞和复杂细胞的经典观念启发得到的，这种细胞是以LGN–V1–V2–V4–IT这种形式形成视觉回路的。当被给予相同图片时，卷积网络可以解释猴子的下颞叶皮质的随机160个神经元的变化。卷积网络有神经认知的根源，有相似的结构，但神经认知中没有类似bp算法之类的端到端的监督学习算法。一个原始的1D卷积网络称作时延神经网络被用于音素和简单单词的识别。 　　



追溯到1990s早期，卷积网络已经有了大量的应用，开始是时延神经网络用于语音识别和文本阅读。文本阅读系统使用一个训练好的卷积网络和一个而受到语言约束的概率模型的二联合。到1990s晚期，这个系统阅读美国所有支票的10%。大量基于卷积网络的视觉特征识别和手写体识别被Microsoft开发。在1990s早期，卷积网络也被实验在人脸，手写图像的目标检测和人脸识别。 



#### 使用深度卷积网络的图像理解　　

自从2000s早期，卷积网络被应用在物体和图像区域的检测，分割和识别，并取得了巨大成功。这些所有的任务都有相对丰富的标签，如交通标志的识别，生物图像的分割和自然图像中的人脸，文本，人类身体的检测。卷积神经网络的一个重大的实践成功是人脸识别。 　　



值得一提的是，图像在像素水平就可以贴上标签，这样就可以应用在自动移动机器人，自动驾驶汽车等。诸如Mobileye and NVIDIA的公司就在即将的到来的汽车的视觉系统中使用了基于卷积神经网络方法。其他值得关注的应用涉及自然语言的理解和语音识别。 　　



尽管有这些成功，卷积神经网络仍然被主流计算机视觉和机器学习团体所抛弃直到2012年的ImageNet竞赛。当深度卷积网络被应用在包含1000个不同类别的大概100万的图像的数据集上时，他们去的了惊人的成功，错误率与最好的竞赛结果的一半接近，这个来源于GPUs，ReLUs，一种称作dropout的规则化技术和通过使现存的图像变形的方法获得更多训练数据的科技。这个成功带来了一场计算机视觉领域的革命；卷积神经网络几乎在所有检测和识别是占优势的方法，并在一些方面性能几乎接近人类。最近，一个很好的证明对于卷积神经网络和递归网络是根据图像产生文本标题（图3）。如图3从图像到文本 ![img](http://images2015.cnblogs.com/blog/920180/201604/920180-20160415194738863-1240271632.png)

使用深度CNN训练测试图像提取表示，再训练RNN翻译，从高水平的表示到标题。当RNN被给予聚焦输入图像的不同的位置的能力时，它产生一个单词（黑体），从而完成翻译的过程。 　　



最近的卷积神经网络有10到20层的ReLUs，亿万个权重，数十亿的节点间连接，2年前，训练这样一个网络需要数周，而现在随着硬件，软件和算法的并行提高，仅仅需要几个小时。 　　



基于卷积神经网络的视觉系统的性能已经引起了许多重大科技公司的关注，包括Google, Facebook, Microsoft, IBM, Yahoo!, Twitter and Adobe,和越来越多的初始公司开始研究并致力于提供基于卷积神经网路的图像理解产品和服务。 　　



卷积神经网络是很容易在芯片或可编程门阵列（FPGA）上实施的。许多的公司例如NVIDIA, Mobileye, Intel, Qualcomm and Samsung正在开发卷积神经网络芯片使之能完成在智能手机，相机，机器人和自动驾驶汽车的实时视觉。 



#### 分布式表示和语音处理

深度学习理论表明深度网络有两个不同的指数优于不使用分布式表示的经典的学习算法,。这两个优点源于组成的权重，并且为依赖于底层的数据生成分布有一个适当的成分结构。首先,分布式表示的学习能概括被学习的特征的新组合 (例如,n个二值特征有2的n次方种可能)。第二,深度网络中的表示的构成层带来了潜在的另一种指数级优势（指数级深度）。 　　



一个多层神经网络的隐形层学习网络输入的特征表示以一种更容易预测目标输出的方式。这很好被证明，通过训练一种多层神经网络去预测文本序列中接下来的的一个单词。内容中的每个单词代表网络中一个N分之1的向量，也就是说，一个成分中有一个1，其他都是0。在第一层中，每个单词创造一个不同模式的激活，或者单词向量（图4）。在一个语言模型中，为了预测下一个单词，网络的其它层学习将输入单词向量转化为一个输出单词向量，这可以通过预测词汇表中作为下一个而单词出现的概率。网络学习包含许多积极成分的单词向量，其中每一个成分都可以作为单词的一个分离特征来解释，正如在学习符号的分布式表示的文本中第一次被证明的的例子一样。这些语义特征没有在输入中被明确的表示。他们在学习过程发现将输入输出符号之间的关系分解为多重“微规则”（‘micro-rules’）可以做为一种好的方式。学习单词向量仍然被证明是性能良好的，当单词序列来自于一个大的现实文本的语料库并且个别的微规则不可靠时。当利用训练好的网络去学习新事例时，概念相似的向量容易混淆，如星期二和星期三，瑞典和挪威。这种表示称为分布式表示，因为他们的元素相互排斥，他们的配置与被观察到的数据的变化一致。这些单词向量由学习好的特征组成，这些特征并不是事先由专家决定，而是由神经网络自动发掘。从文本中学习单词的向量的表示正在自然语言应用中广泛使用。图4可视化被学习的单词向量 

![img](http://images2015.cnblogs.com/blog/920180/201604/920180-20160415195654848-2044944542.png)

左侧是学习单词的模型化语言表示的一个例证，使用t-SNE算法非线性投射到2D空间；右侧是通过English-to-French encoder–decoder递归神将网络学习的短语的2D表示。我们观察到语义相近的词或序列中的单词被映射到附近的表征。分布式表示的单词通过使用反向传播学习每个词的表示和预测目标数量的函数，如序列中下一个单词(用于语言模型)或整个序列（用于机器翻译）。　 　　



特征表示问题存在于逻辑启发和神经网络启发对于范例识别的争论的核心。在逻辑启发范例中，符号的示例具有与其他符号示例完全相同或完全不同的唯一属性。它没有内部结构，并且与它的使用相关，理解它的语义时，它们必须与推断规则变化相绑定。正相反，神经网络使用大的活动向量，大的权重矩阵和标量非线性化来完成快速直观的，支撑简单常识的推断。 　　



在介绍神经语言模型之前，标准的统计语言建模方法并没有开发分布式表示，它是基于长度为N 的短符号序列的计数频率方法（N-grams，N元文法）。可能的N-grams的数目接近V^N,其中V是词汇表的大小，所以考虑上下文可能需要更大的训练库。N-gram每个单词当做一个原子看待，所以不能一般化语义相关单词序列，但是神经语言模型可以，因为他们利用带有实值特征的向量联合单词，再向量空间语义相关的单词是邻近的。 　　



#### 递归神经网络 　　

当bp算法第一次被介绍时，它最激动人心的的使用是训练递归神经网络（RNNs）。RNNs能够更好的处理连续输入如语音和语言（图5）。**RNNs一次处理输入序列中的一个元素，并且将序列过去元素的历史信息含蓄的保存在隐藏节点的状态向量中**。当我们考虑不同离散时间节点隐藏的输出正如深度多层网络不同神经元的输出时，我们能清晰的知道bp算法是如何训练RNNs的。（图5，右）图5 一个递归神经网络和它前向计算在时间上延展

 ![img](http://images2015.cnblogs.com/blog/920180/201604/920180-20160415195842020-1746641549.png)

人造的神经元（例如，隐藏单元的节点s在时间t时有值St）从之前的时间节点的其他神经元获得输入（黑色方块表示一个时延，在右边）。用这种方式，一个递归神经网络可以将输入序列Xt映射到输出序列Ot，每一个Ot取决于之前所有的输入。相同的参数(矩阵U,V,W)在每一个时间节点使用。还可以有许多其他的架构,其中一个变体，网络产生一个输出序列（例如单词），其中每一个作为下步的输入，bp算法可以直接应用于展开网络的计算图表（在右边）,计算总误差（例如，生成正确输出序列的对数概率）对所有状态St和所有参数的导数。 　　



RNNs是非常有力的动态系统，但是训练它们是有问题的，因为反向传播的梯度时增时减，所以一段时间后会激增或将为0。 由于它们的先进结构和训练它们的方式， RNNs是非常善于预测文本中下一个字符或序列中的下一个单词，但是它们也可以用来完成更复杂的任务。例如，在阅读英文句子的单词后，可以训练一个英文“编码器”网络，如此它的隐含层的最终状态向量是句子所表达的意思的好的表示。这个思想向量可以作为训练法语“译码器的”的隐含层的初始状态向量，并输出法语翻译的首单词的概率分布。如果一个特殊的首单词从分布中被选中并做为编码器网络的输入，它将能输出第二个单词的概率分布，如此直到最后。总的来说，根据依赖于英语句子的的概率分布的过程产生了法语单词序列。这种很简单的机器翻译的的方法可以与最先进的方法相媲美，这个引起很大的怀疑对于理解一个句子是否需要使用推理规则操作的内部符号化表达。这与日常推理中同时涉及到根据合适理论类推的观点是相互并列的。 　　



与翻译一个法语句子到英文句子不同，我们也可以学习翻译一幅图像到一个句子（图3）。这的编码器是一个深度卷积网络，在最后的隐藏层将像素转换成活动向量。译码器是与用于机器翻译和神经语言模型的RNN类似的。这样的系统引起了巨大的兴趣。 　　



RNNs一旦在时间上展开，可以看到是一个所有层分享相同权重的深度前向网络（图5）。虽然他们的主要目标是学习长期依赖性，但是**理论和实践都表明它学习储存长期信息是很困难**的。 　　为了修正这个，一个想法是增大网络存储。这种的第一个建议是采用了特殊隐式单元的LSTM,其自然行为就是长时间的保存输入。一个称作记忆细胞的特殊单元，行为像是一个累加器或一个门控神经元：在下一个时间节点时，它与自身以一个权重连接，所以它复制自己的实值状态，并且累加到外部信号中，但是这个自连接是由另一个学习决定何时清除记忆内容的单元以乘法门控制的。 　　



#### LSTM网络

后来被证明是比传统的RNNs更有效，尤其当每一个时间节点有若干层时，整个语音识别系统能够完成从声学到字符序列的转录。LSTM网络或者其他门限单元也被用于编码和译码网络，在机器翻译中表现良好。 在过去，若干学者已经提出了不同的建议对于增大RNNs的记忆模块。建议中包括神经图灵机，其中通过加入RNNs可选择读或写的“类似磁带”的存储来增强网络，而记忆网络中的常规网络通过联想记忆来增强。记忆网络在标准的问答基准测试中表现良好，记忆是用来记住后来被要求回答的问题的事例。 　　



除了简单的记忆，**神经图灵机和记忆网络**还被用于推理或符号操作的任务。神经图灵机也以被教授算法。在其它的事情中，当它们的输入由未排序的序列的组成时，它们可以学习输出符号的排序后的列表，如此每个符号都会有一个表明优先值的实值。记忆网络可以被训练去追踪一个与文字冒险游戏相似的世界，并且在阅读一个故事后，可以回答要求复杂推理的问题。在一个测试示例中，网络被给予一个15句版本的《指环王》，并正确回答例如“where is Frodo now?” 



#### 深度学习展望　　

监督学习对于恢复人们对于深度学习的兴趣有一个催化剂的作用，但是纯粹的监督学习的成功已经失去光彩了。尽管我们在这篇综述中没有关注太多**无监督学习**，但是从长期来看期望它变得更加重要。人类和动物学习是无监督的：我们通过观察世界发现世界的结构，而不是被告知每个物体的名字。 　　



人类视觉是通过使用小的，高分辨率的视网膜中间凹和大的低分辨率的周围以一种智能的，特定方式的采集成像的活跃的过程。我们期望未来的发展前景来自于端到端的训练，并且**联合ConvNets 和使用强化学习的 RNNs从而决定去哪里看**。联合深度学习和强化学习的系统即使在初级阶段，在分类任务上效果也会超越被动视觉系统，并且在学习播放许多不同的音频游戏时产生令人印象深刻的结果。 　　



**自然语言理解**是另一个在深度学习中准备去做，并将在未来几年产生大的影响的想法。我们希望使用RNNs理解句子或整个文档的系统可以变得更好，当有策略的学习选择性一次加入某一部分。 　　



最后，在**人工智能**将会取得重大进展，通过**结合表示学习和复杂推理**。尽管深度学习和简单的推理用于语音和手写识别很长一段时间,但是新的范例被需要通过操作大型向量去替代基于规则的符号表示操作。 



